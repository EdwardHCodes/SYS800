
 * Copyright (c) 2014 - 2016, The Linux Foundation. All rights reserved.
 *
 * Permission to use, copy, modify, and/or distribute this software for
 * any purpose with or without fee is hereby granted, provided that the
 * above copyright notice and this permission notice appear in all copies.
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
  edma_skb_priority_offset()
 * 	get edma skb priority
  edma_alloc_tx_ring()
 *	Allocate Tx descriptors ring
  Initialize ring  Allocate SW descriptors  Allocate HW descriptors  edma_free_tx_ring()
 *	Free tx rings allocated by edma_alloc_tx_rings
  edma_alloc_rx_ring()
 *	allocate rx descriptor ring
  Allocate SW descriptors  Alloc HW descriptors  Initialize pending_fill  edma_free_rx_ring()
 *	Free rx ring allocated by alloc_rx_ring
  edma_configure_tx()
 *	Configure transmission control data
  edma_configure_rx()
 *	configure reception control data
  Set RSS type  Set RFD burst number  Set RFD prefetch threshold  Set RFD in host ring low threshold to generte interrupt  Set Rx FIFO threshold to start to DMA data to host  Set RX remove vlan bit  edma_alloc_rx_buf()
 *	does skb allocation for the received packets.
  Clear REUSE Flag  alloc skb  Better luck next round  Update the buffer info  Update the producer index  If we couldn't allocate all the buffers
	 * we increment the alloc failure counters
	  edma_init_desc()
 *	update descriptor ring size, buffer and producer/consumer index
  Set the base address of every TPD ring.  Update descriptor ring base address  Calculate hardware consumer index  update producer index  update SW consumer index register  Set TPD ring size  Update Receive Free descriptor ring base address  Update RFD ring size and RX buffer size  Disable TX FIFO low watermark and high watermark  Load all of base address above  edma_receive_checksum
 *	Api to check checksum on receive packets
  check the RRD IP/L4 checksum bit to see if
	 * its set, which in turn indicates checksum
	 * failure.
	  edma_clean_rfd()
 *	clean up rx resourcers on error
  edma_rx_complete_fraglist()
 *	Complete Rx processing for fraglist skbs
  if port type is 0x4, then only proceed with
	 * other stp/rstp calculation
	  calculate the frame priority  Check if destination mac addr is bpdu addr  destination mac address is BPDU
			 * destination mac address, then add
			 * atheros header to the packet.
			 
 * edma_rx_complete_fraglist()
 *	Complete Rx processing for fraglist skbs
  clean-up all related sw_descs 
		 * If we are processing the first rfd, we link
		 * skb->frag_list to the skb corresponding to the
		 * first RFD
		  Increment SW index  edma_rx_complete_paged()
 *	Complete Rx processing for paged skbs
  Setup skbuff fields  clean-up all related sw_descs  Increment SW index 
 * edma_rx_complete()
 *	Main api called from the poll function to process rx packets.
  Unmap the allocated buffer  Get RRD  Check if RRD is valid  Get the number of RFDs from RRD  Get Rx port ID from switch  check if we have a sink for the data we receive.
			 * If the interface isn't setup, we have to drop the
			 * incoming data for now.
			  This code is added to handle a usecase where high
			 * priority stream and a low priority stream are
			 * received simultaneously on DUT. The problem occurs
			 * if one of the  Rx rings is full and the corresponding
			 * core is busy with other stuff. This causes ESS CPU
			 * port to backpressure all incoming traffic including
			 * high priority one. We monitor free descriptor count
			 * on each CPU and whenever it reaches threshold (< 80),
			 * we drop all low priority traffic and let only high
			 * priotiy traffic pass through. We can hence avoid
			 * ESS CPU port to send backpressure on high priroity
			 * stream.
			  If buffer clean count reaches 16, we replenish HW buffers.  Increment SW index  Get the packet size and allocate buffer  paged skb  single or fraglist skb  Addition of 16 bytes is required, as in the packet
				 * first 16 bytes are rrd descriptors, so actual data
				 * starts from an offset of 16.
				  Record Rx queue for RFS/RPS and fill flow hash from HW  Process VLAN HW acceleration indication provided by HW  Update rx statistics  Check if we reached refill threshold  At this point skb should go to stack  Check if we still have NAPI budget  Read index once again since we still have NAPI budget  Refill here in case refill threshold wasn't reached  edma_delete_rfs_filter()
 *	Remove RFS filter from switch
  edma_add_rfs_filter()
 *	Add RFS filter to switch
 
	dest_keys->control = keys->control;
	dest_keys->basic = keys->basic;
	dest_keys->addrs = keys->addrs;
	dest_keys->ports = keys->ports;
	dest_keys.ip_proto = keys->ip_proto;
 Call callback registered by ESS driver  edma_rfs_key_search()
 *	Look for existing RFS entry
  edma_initialise_rfs_flow_table()
 * 	Initialise EDMA RFS flow table
  Initialize EDMA flow hash table  Add timer to get periodic RFS updates from OS  edma_free_rfs_flow_table()
 * 	Free EDMA RFS flow table
  Remove sync timer  Free EDMA RFS table entries  Clean-up EDMA flow hash table  edma_tx_unmap_and_free()
 *	clean TX buffer
  unmap_single for skb head area  unmap page for paged fragments  edma_tx_complete()
 *	Used to clean tx queues and update hardware and consumer index
  clean the buffer here  update the TPD consumer index register  Wake the queue if queue is stopped and netdev link is up  edma_get_tx_buffer()
 *	Get sw_desc corresponding to the TPD
  edma_get_next_tpd()
 *	Return a TPD descriptor for transfer
  edma_tpd_available()
 *	Check number of free TPDs
  edma_tx_queue_get()
 *	Get the starting number of  the queue
  skb->priority is used as an index to skb priority table
	 * and based on packet priority, correspong queue is assigned.
	  edma_tx_update_hw_idx()
 *	update the producer index for the ring transmitted
  Read and update the producer index  edma_rollback_tx()
 *	Function to retrieve tx resources in case of error
  edma_tx_map_and_fill()
 *	gets called from edma_xmit_frame
 *
 * This is where the dma of the buffer to be transmitted
 * gets mapped
  It should either be a nr_frags skb or fraglist skb but not both  TODO: What additional checks need to be performed here  IPv6 LSOv2 descriptor  LSOv2 descriptor overrides addr field to pass length  The last buffer info contain the skb address,
		 * so skb will be freed after unmap
		  The last buffer info contain the skb address,
		 * so it will be freed after unmap
		  TODO Do not dequeue descriptor if there is a potential error  The last buffer info contain the skb address,
		 * so it will be free after unmap
		  Walk through all paged fragments  Walk through all fraglist skbs  edma_check_link()
 *	check Link status
  edma_adjust_link()
 *	check for edma link status
  edma_get_stats()
 *	Statistics api used to retreive the tx/rx statistics
  edma_xmit()
 *	Main api to be called by the core for packet transmission
  this will be one of the 4 TX queues exposed to linux kernel  Tx is not handled in bottom half context. Hence, we need to protect
	 * Tx from tasks and bottom half
	  not enough descriptor, just stop queue  Check and mark VLAN tag offload  Check and mark checksum offload  Map and fill descriptor for Tx  Update SW producer index  update tx statistics 
 * edma_flow_may_expire()
 * 	Timer function called periodically to delete the node
  edma_rx_flow_steer()
 *	Called by core to to steer the flow to CPU
  Dissect flow parameters
	 * We only support IPv4 + TCP/UDP
	  Check if table entry exists  edma_register_rfs_filter()
 *	Add RFS filter callback
  edma_alloc_tx_rings()
 *	Allocate rx rings
  edma_free_tx_rings()
 *	Free tx rings
  edma_free_tx_resources()
 *	Free buffers associated with tx rings
  edma_alloc_rx_rings()
 *	Allocate rx rings
  edma_free_rx_rings()
 *	free rx rings
  edma_free_queues()
 *	Free the queues allocaated
  edma_free_rx_resources()
 *	Free buffers associated with tx rings
  edma_alloc_queues_tx()
 *	Allocate memory for all rings
  edma_alloc_queues_rx()
 *	Allocate memory for all rings
  edma_clear_irq_status()
 *	Clear interrupt status
  edma_configure()
 *	Configure skb, edma interrupts and control register.
  Clear any WOL status  Allocate the RX buffer  Configure descriptor Ring  edma_irq_enable()
 *	Enable default interrupt generation settings
  edma_irq_disable()
 *	Disable Interrupt
  edma_free_irqs()
 *	Free All IRQs
  edma_enable_rx_ctrl()
 *	Enable RX queue control
  edma_enable_tx_ctrl()
 *	Enable TX queue control
  edma_stop_rx_tx()
 *	Disable RX/TQ Queue control
  edma_reset()
 *	Reset the EDMA
  edma_fill_netdev()
 * 	Fill netdev for each etdr
  Populate the netdev associated with the tpd ring  edma_set_mac()
 *	Change the Ethernet Address of the NIC
  edma_set_stp_rstp()
 *	set stp/rstp
  edma_assign_ath_hdr_type()
 *	assign atheros header eth type
  edma_get_default_vlan_tag()
 *	Used by other modules to get the default vlan tag
  edma_open()
 *	gets called when netdevice is up, start the queue.
  if Link polling is enabled, in our case enabled for WAN, then
	 * do a phy start, else always set link as UP
	  edma_close()
 *	gets called when netdevice is down, stops the queue.
  Set GMAC state to UP before link state is checked
	  edma_poll
 *	polling function that gets called when the napi gets scheduled.
 *
 * Main sequence of task performed in this api
 * is clear irq status -> clear_tx_irq -> clean_rx_irq->
 * enable interrupts.
  Store the Rx/Tx status by ANDing it with
	 * appropriate CPU RX?TX mask
	  Every core will have a start, which will be computed
	 * in probe and stored in edma_percpu_info->tx_start variable.
	 * We will shift the status bit by tx_start to obtain
	 * status bits for the core on which the current processing
	 * is happening. Since, there are 4 tx queues per core,
	 * we will run the loop till we get the correct queue to clear.
	  Every core will have a start, which will be computed
	 * in probe and stored in edma_percpu_info->tx_start variable.
	 * We will shift the status bit by tx_start to obtain
	 * status bits for the core on which the current processing
	 * is happening. Since, there are 4 tx queues per core, we
	 * will run the loop till we get the correct queue to clear.
	  reschedule poll() to refill rx buffer deficit  Clear the status register, to avoid the interrupts to
	 * reoccur.This clearing of interrupt status register is
	 * done here as writing to status register only takes place
	 * once the  producer/consumer index has been updated to
	 * reflect that the packet transmission/reception went fine.
	  If budget not fully consumed, exit the polling mode  re-enable the interrupts  edma interrupt()
 *	interrupt handler
  Unmask the TX/RX interrupt register 
 * Copyright (c) 2014 - 2016, The Linux Foundation. All rights reserved.
 *
 * Permission to use, copy, modify, and/or distribute this software for
 * any purpose with or without fee is hereby granted, provided that the
 * above copyright notice and this permission notice appear in all copies.
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
  edma_skb_priority_offset()
 * 	get edma skb priority
  edma_alloc_tx_ring()
 *	Allocate Tx descriptors ring
  Initialize ring  Allocate SW descriptors  Allocate HW descriptors  edma_free_tx_ring()
 *	Free tx rings allocated by edma_alloc_tx_rings
  edma_alloc_rx_ring()
 *	allocate rx descriptor ring
  Allocate SW descriptors  Alloc HW descriptors  Initialize pending_fill  edma_free_rx_ring()
 *	Free rx ring allocated by alloc_rx_ring
  edma_configure_tx()
 *	Configure transmission control data
  edma_configure_rx()
 *	configure reception control data
  Set RSS type  Set RFD burst number  Set RFD prefetch threshold  Set RFD in host ring low threshold to generte interrupt  Set Rx FIFO threshold to start to DMA data to host  Set RX remove vlan bit  edma_alloc_rx_buf()
 *	does skb allocation for the received packets.
  Clear REUSE Flag  alloc skb  Better luck next round  Update the buffer info  Update the producer index  If we couldn't allocate all the buffers
	 * we increment the alloc failure counters
	  edma_init_desc()
 *	update descriptor ring size, buffer and producer/consumer index
  Set the base address of every TPD ring.  Update descriptor ring base address  Calculate hardware consumer index  update producer index  update SW consumer index register  Set TPD ring size  Update Receive Free descriptor ring base address  Update RFD ring size and RX buffer size  Disable TX FIFO low watermark and high watermark  Load all of base address above  edma_receive_checksum
 *	Api to check checksum on receive packets
  check the RRD IP/L4 checksum bit to see if
	 * its set, which in turn indicates checksum
	 * failure.
	  edma_clean_rfd()
 *	clean up rx resourcers on error
  edma_rx_complete_fraglist()
 *	Complete Rx processing for fraglist skbs
  if port type is 0x4, then only proceed with
	 * other stp/rstp calculation
	  calculate the frame priority  Check if destination mac addr is bpdu addr  destination mac address is BPDU
			 * destination mac address, then add
			 * atheros header to the packet.
			 
 * edma_rx_complete_fraglist()
 *	Complete Rx processing for fraglist skbs
  clean-up all related sw_descs 
		 * If we are processing the first rfd, we link
		 * skb->frag_list to the skb corresponding to the
		 * first RFD
		  Increment SW index  edma_rx_complete_paged()
 *	Complete Rx processing for paged skbs
  Setup skbuff fields  clean-up all related sw_descs  Increment SW index 
 * edma_rx_complete()
 *	Main api called from the poll function to process rx packets.
  Unmap the allocated buffer  Get RRD  Check if RRD is valid  Get the number of RFDs from RRD  Get Rx port ID from switch  check if we have a sink for the data we receive.
			 * If the interface isn't setup, we have to drop the
			 * incoming data for now.
			  This code is added to handle a usecase where high
			 * priority stream and a low priority stream are
			 * received simultaneously on DUT. The problem occurs
			 * if one of the  Rx rings is full and the corresponding
			 * core is busy with other stuff. This causes ESS CPU
			 * port to backpressure all incoming traffic including
			 * high priority one. We monitor free descriptor count
			 * on each CPU and whenever it reaches threshold (< 80),
			 * we drop all low priority traffic and let only high
			 * priotiy traffic pass through. We can hence avoid
			 * ESS CPU port to send backpressure on high priroity
			 * stream.
			  If buffer clean count reaches 16, we replenish HW buffers.  Increment SW index  Get the packet size and allocate buffer  paged skb  single or fraglist skb  Addition of 16 bytes is required, as in the packet
				 * first 16 bytes are rrd descriptors, so actual data
				 * starts from an offset of 16.
				  Record Rx queue for RFS/RPS and fill flow hash from HW  Process VLAN HW acceleration indication provided by HW  Update rx statistics  Check if we reached refill threshold  At this point skb should go to stack  Check if we still have NAPI budget  Read index once again since we still have NAPI budget  Refill here in case refill threshold wasn't reached  edma_delete_rfs_filter()
 *	Remove RFS filter from switch
  edma_add_rfs_filter()
 *	Add RFS filter to switch
 
	dest_keys->control = keys->control;
	dest_keys->basic = keys->basic;
	dest_keys->addrs = keys->addrs;
	dest_keys->ports = keys->ports;
	dest_keys.ip_proto = keys->ip_proto;
 Call callback registered by ESS driver  edma_rfs_key_search()
 *	Look for existing RFS entry
  edma_initialise_rfs_flow_table()
 * 	Initialise EDMA RFS flow table
  Initialize EDMA flow hash table  Add timer to get periodic RFS updates from OS  edma_free_rfs_flow_table()
 * 	Free EDMA RFS flow table
  Remove sync timer  Free EDMA RFS table entries  Clean-up EDMA flow hash table  edma_tx_unmap_and_free()
 *	clean TX buffer
  unmap_single for skb head area  unmap page for paged fragments  edma_tx_complete()
 *	Used to clean tx queues and update hardware and consumer index
  clean the buffer here  update the TPD consumer index register  Wake the queue if queue is stopped and netdev link is up  edma_get_tx_buffer()
 *	Get sw_desc corresponding to the TPD
  edma_get_next_tpd()
 *	Return a TPD descriptor for transfer
  edma_tpd_available()
 *	Check number of free TPDs
  edma_tx_queue_get()
 *	Get the starting number of  the queue
  skb->priority is used as an index to skb priority table
	 * and based on packet priority, correspong queue is assigned.
	  edma_tx_update_hw_idx()
 *	update the producer index for the ring transmitted
  Read and update the producer index  edma_rollback_tx()
 *	Function to retrieve tx resources in case of error
  edma_tx_map_and_fill()
 *	gets called from edma_xmit_frame
 *
 * This is where the dma of the buffer to be transmitted
 * gets mapped
  It should either be a nr_frags skb or fraglist skb but not both  TODO: What additional checks need to be performed here  IPv6 LSOv2 descriptor  LSOv2 descriptor overrides addr field to pass length  The last buffer info contain the skb address,
		 * so skb will be freed after unmap
		  The last buffer info contain the skb address,
		 * so it will be freed after unmap
		  TODO Do not dequeue descriptor if there is a potential error  The last buffer info contain the skb address,
		 * so it will be free after unmap
		  Walk through all paged fragments  Walk through all fraglist skbs  edma_check_link()
 *	check Link status
  edma_adjust_link()
 *	check for edma link status
  edma_get_stats()
 *	Statistics api used to retreive the tx/rx statistics
  edma_xmit()
 *	Main api to be called by the core for packet transmission
  this will be one of the 4 TX queues exposed to linux kernel  Tx is not handled in bottom half context. Hence, we need to protect
	 * Tx from tasks and bottom half
	  not enough descriptor, just stop queue  Check and mark VLAN tag offload  Check and mark checksum offload  Map and fill descriptor for Tx  Update SW producer index  update tx statistics 
 * edma_flow_may_expire()
 * 	Timer function called periodically to delete the node
  edma_rx_flow_steer()
 *	Called by core to to steer the flow to CPU
  Dissect flow parameters
	 * We only support IPv4 + TCP/UDP
	  Check if table entry exists  edma_register_rfs_filter()
 *	Add RFS filter callback
  edma_alloc_tx_rings()
 *	Allocate rx rings
  edma_free_tx_rings()
 *	Free tx rings
  edma_free_tx_resources()
 *	Free buffers associated with tx rings
  edma_alloc_rx_rings()
 *	Allocate rx rings
  edma_free_rx_rings()
 *	free rx rings
  edma_free_queues()
 *	Free the queues allocaated
  edma_free_rx_resources()
 *	Free buffers associated with tx rings
  edma_alloc_queues_tx()
 *	Allocate memory for all rings
  edma_alloc_queues_rx()
 *	Allocate memory for all rings
  edma_clear_irq_status()
 *	Clear interrupt status
  edma_configure()
 *	Configure skb, edma interrupts and control register.
  Clear any WOL status  Allocate the RX buffer  Configure descriptor Ring  edma_irq_enable()
 *	Enable default interrupt generation settings
  edma_irq_disable()
 *	Disable Interrupt
  edma_free_irqs()
 *	Free All IRQs
  edma_enable_rx_ctrl()
 *	Enable RX queue control
  edma_enable_tx_ctrl()
 *	Enable TX queue control
  edma_stop_rx_tx()
 *	Disable RX/TQ Queue control
  edma_reset()
 *	Reset the EDMA
  edma_fill_netdev()
 * 	Fill netdev for each etdr
  Populate the netdev associated with the tpd ring  edma_set_mac()
 *	Change the Ethernet Address of the NIC
  edma_set_stp_rstp()
 *	set stp/rstp
  edma_assign_ath_hdr_type()
 *	assign atheros header eth type
  edma_get_default_vlan_tag()
 *	Used by other modules to get the default vlan tag
  edma_open()
 *	gets called when netdevice is up, start the queue.
  if Link polling is enabled, in our case enabled for WAN, then
	 * do a phy start, else always set link as UP
	  edma_close()
 *	gets called when netdevice is down, stops the queue.
  Set GMAC state to UP before link state is checked
	  edma_poll
 *	polling function that gets called when the napi gets scheduled.
 *
 * Main sequence of task performed in this api
 * is clear irq status -> clear_tx_irq -> clean_rx_irq->
 * enable interrupts.
  Store the Rx/Tx status by ANDing it with
	 * appropriate CPU RX?TX mask
	  Every core will have a start, which will be computed
	 * in probe and stored in edma_percpu_info->tx_start variable.
	 * We will shift the status bit by tx_start to obtain
	 * status bits for the core on which the current processing
	 * is happening. Since, there are 4 tx queues per core,
	 * we will run the loop till we get the correct queue to clear.
	  Every core will have a start, which will be computed
	 * in probe and stored in edma_percpu_info->tx_start variable.
	 * We will shift the status bit by tx_start to obtain
	 * status bits for the core on which the current processing
	 * is happening. Since, there are 4 tx queues per core, we
	 * will run the loop till we get the correct queue to clear.
	  reschedule poll() to refill rx buffer deficit  Clear the status register, to avoid the interrupts to
	 * reoccur.This clearing of interrupt status register is
	 * done here as writing to status register only takes place
	 * once the  producer/consumer index has been updated to
	 * reflect that the packet transmission/reception went fine.
	  If budget not fully consumed, exit the polling mode  re-enable the interrupts  edma interrupt()
 *	interrupt handler
  Unmask the TX/RX interrupt register 
 * Copyright (c) 2014 - 2016, The Linux Foundation. All rights reserved.
 *
 * Permission to use, copy, modify, and/or distribute this software for
 * any purpose with or without fee is hereby granted, provided that the
 * above copyright notice and this permission notice appear in all copies.
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
 * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
  edma_skb_priority_offset()
 * 	get edma skb priority
  edma_alloc_tx_ring()
 *	Allocate Tx descriptors ring
  Initialize ring  Allocate SW descriptors  Allocate HW descriptors  edma_free_tx_ring()
 *	Free tx rings allocated by edma_alloc_tx_rings
  edma_alloc_rx_ring()
 *	allocate rx descriptor ring
  Allocate SW descriptors  Alloc HW descriptors  Initialize pending_fill  edma_free_rx_ring()
 *	Free rx ring allocated by alloc_rx_ring
  edma_configure_tx()
 *	Configure transmission control data
  edma_configure_rx()
 *	configure reception control data
  Set RSS type  Set RFD burst number  Set RFD prefetch threshold  Set RFD in host ring low threshold to generte interrupt  Set Rx FIFO threshold to start to DMA data to host  Set RX remove vlan bit  edma_alloc_rx_buf()
 *	does skb allocation for the received packets.
  Clear REUSE Flag  alloc skb  Better luck next round  Update the buffer info  Update the producer index  If we couldn't allocate all the buffers
	 * we increment the alloc failure counters
	  edma_init_desc()
 *	update descriptor ring size, buffer and producer/consumer index
  Set the base address of every TPD ring.  Update descriptor ring base address  Calculate hardware consumer index  update producer index  update SW consumer index register  Set TPD ring size  Update Receive Free descriptor ring base address  Update RFD ring size and RX buffer size  Disable TX FIFO low watermark and high watermark  Load all of base address above  edma_receive_checksum
 *	Api to check checksum on receive packets
  check the RRD IP/L4 checksum bit to see if
	 * its set, which in turn indicates checksum
	 * failure.
	  edma_clean_rfd()
 *	clean up rx resourcers on error
  edma_rx_complete_fraglist()
 *	Complete Rx processing for fraglist skbs
  if port type is 0x4, then only proceed with
	 * other stp/rstp calculation
	  calculate the frame priority  Check if destination mac addr is bpdu addr  destination mac address is BPDU
			 * destination mac address, then add
			 * atheros header to the packet.
			 
 * edma_rx_complete_fraglist()
 *	Complete Rx processing for fraglist skbs
  clean-up all related sw_descs 
		 * If we are processing the first rfd, we link
		 * skb->frag_list to the skb corresponding to the
		 * first RFD
		  Increment SW index  edma_rx_complete_paged()
 *	Complete Rx processing for paged skbs
  Setup skbuff fields  clean-up all related sw_descs  Increment SW index 
 * edma_rx_complete()
 *	Main api called from the poll function to process rx packets.
  Unmap the allocated buffer  Get RRD  Check if RRD is valid  Get the number of RFDs from RRD  Get Rx port ID from switch  check if we have a sink for the data we receive.
			 * If the interface isn't setup, we have to drop the
			 * incoming data for now.
			  This code is added to handle a usecase where high
			 * priority stream and a low priority stream are
			 * received simultaneously on DUT. The problem occurs
			 * if one of the  Rx rings is full and the corresponding
			 * core is busy with other stuff. This causes ESS CPU
			 * port to backpressure all incoming traffic including
			 * high priority one. We monitor free descriptor count
			 * on each CPU and whenever it reaches threshold (< 80),
			 * we drop all low priority traffic and let only high
			 * priotiy traffic pass through. We can hence avoid
			 * ESS CPU port to send backpressure on high priroity
			 * stream.
			  If buffer clean count reaches 16, we replenish HW buffers.  Increment SW index  Get the packet size and allocate buffer  paged skb  single or fraglist skb  Addition of 16 bytes is required, as in the packet
				 * first 16 bytes are rrd descriptors, so actual data
				 * starts from an offset of 16.
				  Record Rx queue for RFS/RPS and fill flow hash from HW  Process VLAN HW acceleration indication provided by HW  Update rx statistics  Check if we reached refill threshold  At this point skb should go to stack  Check if we still have NAPI budget  Read index once again since we still have NAPI budget  Refill here in case refill threshold wasn't reached  edma_delete_rfs_filter()
 *	Remove RFS filter from switch
  edma_add_rfs_filter()
 *	Add RFS filter to switch
 
	dest_keys->control = keys->control;
	dest_keys->basic = keys->basic;
	dest_keys->addrs = keys->addrs;
	dest_keys->ports = keys->ports;
	dest_keys.ip_proto = keys->ip_proto;
 Call callback registered by ESS driver  edma_rfs_key_search()
 *	Look for existing RFS entry
  edma_initialise_rfs_flow_table()
 * 	Initialise EDMA RFS flow table
  Initialize EDMA flow hash table  Add timer to get periodic RFS updates from OS  edma_free_rfs_flow_table()
 * 	Free EDMA RFS flow table
  Remove sync timer  Free EDMA RFS table entries  Clean-up EDMA flow hash table  edma_tx_unmap_and_free()
 *	clean TX buffer
  unmap_single for skb head area  unmap page for paged fragments  edma_tx_complete()
 *	Used to clean tx queues and update hardware and consumer index
  clean the buffer here  update the TPD consumer index register  Wake the queue if queue is stopped and netdev link is up  edma_get_tx_buffer()
 *	Get sw_desc corresponding to the TPD
  edma_get_next_tpd()
 *	Return a TPD descriptor for transfer
  edma_tpd_available()
 *	Check number of free TPDs
  edma_tx_queue_get()
 *	Get the starting number of  the queue
  skb->priority is used as an index to skb priority table
	 * and based on packet priority, correspong queue is assigned.
	  edma_tx_update_hw_idx()
 *	update the producer index for the ring transmitted
  Read and update the producer index  edma_rollback_tx()
 *	Function to retrieve tx resources in case of error
  edma_tx_map_and_fill()
 *	gets called from edma_xmit_frame
 *
 * This is where the dma of the buffer to be transmitted
 * gets mapped
  It should either be a nr_frags skb or fraglist skb but not both  TODO: What additional checks need to be performed here  IPv6 LSOv2 descriptor  LSOv2 descriptor overrides addr field to pass length  The last buffer info contain the skb address,
		 * so skb will be freed after unmap
		  The last buffer info contain the skb address,
		 * so it will be freed after unmap
		  TODO Do not dequeue descriptor if there is a potential error  The last buffer info contain the skb address,
		 * so it will be free after unmap
		  Walk through all paged fragments  Walk through all fraglist skbs  edma_check_link()
 *	check Link status
  edma_adjust_link()
 *	check for edma link status
  edma_get_stats()
 *	Statistics api used to retreive the tx/rx statistics
  edma_xmit()
 *	Main api to be called by the core for packet transmission
  this will be one of the 4 TX queues exposed to linux kernel  Tx is not handled in bottom half context. Hence, we need to protect
	 * Tx from tasks and bottom half
	  not enough descriptor, just stop queue  Check and mark VLAN tag offload  Check and mark checksum offload  Map and fill descriptor for Tx  Update SW producer index  update tx statistics 
 * edma_flow_may_expire()
 * 	Timer function called periodically to delete the node
  edma_rx_flow_steer()
 *	Called by core to to steer the flow to CPU
  Dissect flow parameters
	 * We only support IPv4 + TCP/UDP
	  Check if table entry exists  edma_register_rfs_filter()
 *	Add RFS filter callback
  edma_alloc_tx_rings()
 *	Allocate rx rings
  edma_free_tx_rings()
 *	Free tx rings
  edma_free_tx_resources()
 *	Free buffers associated with tx rings
  edma_alloc_rx_rings()
 *	Allocate rx rings
  edma_free_rx_rings()
 *	free rx rings
  edma_free_queues()
 *	Free the queues allocaated
  edma_free_rx_resources()
 *	Free buffers associated with tx rings
  edma_alloc_queues_tx()
 *	Allocate memory for all rings
  edma_alloc_queues_rx()
 *	Allocate memory for all rings
  edma_clear_irq_status()
 *	Clear interrupt status
  edma_configure()
 *	Configure skb, edma interrupts and control register.
  Clear any WOL status  Allocate the RX buffer  Configure descriptor Ring  edma_irq_enable()
 *	Enable default interrupt generation settings
  edma_irq_disable()
 *	Disable Interrupt
  edma_free_irqs()
 *	Free All IRQs
  edma_enable_rx_ctrl()
 *	Enable RX queue control
  edma_enable_tx_ctrl()
 *	Enable TX queue control
  edma_stop_rx_tx()
 *	Disable RX/TQ Queue control
  edma_reset()
 *	Reset the EDMA
  edma_fill_netdev()
 * 	Fill netdev for each etdr
  Populate the netdev associated with the tpd ring  edma_set_mac()
 *	Change the Ethernet Address of the NIC
  edma_set_stp_rstp()
 *	set stp/rstp
  edma_assign_ath_hdr_type()
 *	assign atheros header eth type
  edma_get_default_vlan_tag()
 *	Used by other modules to get the default vlan tag
  edma_open()
 *	gets called when netdevice is up, start the queue.
  if Link polling is enabled, in our case enabled for WAN, then
	 * do a phy start, else always set link as UP
	  edma_close()
 *	gets called when netdevice is down, stops the queue.
  Set GMAC state to UP before link state is checked
	  edma_poll
 *	polling function that gets called when the napi gets scheduled.
 *
 * Main sequence of task performed in this api
 * is clear irq status -> clear_tx_irq -> clean_rx_irq->
 * enable interrupts.
  Store the Rx/Tx status by ANDing it with
	 * appropriate CPU RX?TX mask
	  Every core will have a start, which will be computed
	 * in probe and stored in edma_percpu_info->tx_start variable.
	 * We will shift the status bit by tx_start to obtain
	 * status bits for the core on which the current processing
	 * is happening. Since, there are 4 tx queues per core,
	 * we will run the loop till we get the correct queue to clear.
	  Every core will have a start, which will be computed
	 * in probe and stored in edma_percpu_info->tx_start variable.
	 * We will shift the status bit by tx_start to obtain
	 * status bits for the core on which the current processing
	 * is happening. Since, there are 4 tx queues per core, we
	 * will run the loop till we get the correct queue to clear.
	  reschedule poll() to refill rx buffer deficit  Clear the status register, to avoid the interrupts to
	 * reoccur.This clearing of interrupt status register is
	 * done here as writing to status register only takes place
	 * once the  producer/consumer index has been updated to
	 * reflect that the packet transmission/reception went fine.
	  If budget not fully consumed, exit the polling mode  re-enable the interrupts  edma interrupt()
 *	interrupt handler
  Unmask the TX/RX interrupt register 